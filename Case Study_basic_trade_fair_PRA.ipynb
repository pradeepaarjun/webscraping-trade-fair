{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective \n",
    "\n",
    "The below code is to collect the information for each company Trade Fair from the website https://london.vetshow.com/exhibitor-list. Total companies obtained from web scraping is 428.\n",
    "\n",
    "· Company Name\n",
    "\n",
    "· Description (If Any)\n",
    "\n",
    "· Stand (If Any)\n",
    "\n",
    "### Virtual Python environment with conda\n",
    "\n",
    "Created a new virtual environment named case-study with the version Python 3.9 with the below command. The purpose of using virtual environment is to install packages specifically for the project that we are working on.\n",
    "\n",
    "$ conda create -y python=3.9 --name case-study\n",
    "\n",
    "Created a requirements.txt file in the root and added the installed package. Easy to check what packages are required and what versions they need to be. \n",
    "\n",
    "$ pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the exhibitor list and company pages\n",
    "main_base_url = \"https://london.vetshow.com/exhibitor-list?page=\"\n",
    "company_base_url = \"https://london.vetshow.com/\"\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Step 1: Fetch the first page to determine the total number of pages\n",
    "response = requests.get(main_base_url + \"1\", headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pages found: 44\n"
     ]
    }
   ],
   "source": [
    "# Find the pagination section and get the last page number\n",
    "pagination = soup.find('ul', class_=\"pagination__list\")\n",
    "last_page_link = pagination.find_all('a', element=\"pages\")[-1]\n",
    "total_pages = int(last_page_link.get('data-page'))\n",
    "\n",
    "print(f\"Total number of pages found: {total_pages}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold each company's data across all pages\n",
    "companies_data = []\n",
    "\n",
    "# Step 2: Looping through each page to gather company data and links to details\n",
    "for page in range(1, total_pages + 1):\n",
    "    url = f\"{main_base_url}{page}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Locate each company on the current page\n",
    "    company_list = soup.find_all('li', class_=\"m-exhibitors-list__items__item\")\n",
    "\n",
    "    for company in company_list:\n",
    "        # Extract Company Name\n",
    "        company_name = company.find(\"h2\", class_=\"m-exhibitors-list__items__item__header__title\").get_text(strip=True)\n",
    "\n",
    "        # Extract Description (truncated on main page)\n",
    "        description = company.find(\"div\", class_=\"m-exhibitors-list__items__item__body__description\")\n",
    "        description_text = description.get_text(strip=True) if description else None\n",
    "\n",
    "        # Extract Stand (if available)\n",
    "        stand = company.find(\"div\", class_=\"m-exhibitors-list__items__item__header__meta__stand\")\n",
    "        stand_text = stand.get_text(strip=True).replace(\"Stand:\", \"\").strip() if stand else None\n",
    "        if description_text is not None:\n",
    "        # Extract Company Page Link\n",
    "            company_relative_link = company.find(\"a\", class_=\"js-librarylink-entry\").get(\"href\")\n",
    "            company_full_url = company_base_url + company_relative_link\n",
    "        # Fetch the complete description from the company's individual page if there is description\n",
    "            company_response = requests.get(company_full_url, headers=headers)\n",
    "            company_soup = BeautifulSoup(company_response.text, 'html.parser')\n",
    "\n",
    "            # Locate the full description\n",
    "            full_description = company_soup.find(\"div\", class_=\"m-exhibitor-entry__item__body__description\")\n",
    "            description_text = full_description.get_text(strip=True) if full_description else description_text\n",
    "\n",
    "        # Append all collected data for each company\n",
    "        companies_data.append({\n",
    "            \"Company Name\": company_name,\n",
    "            \"Description\": description_text,\n",
    "            \"Stand\": stand_text\n",
    "        })\n",
    "\n",
    "        # Adding a delay to avoid overwhelming the server\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped successfully from 44 pages and saved to LondonVetShow_Exhibitors_Full_Description.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Save the data to an Excel file\n",
    "df = pd.DataFrame(companies_data)\n",
    "output_file = \"LondonVetShow_Exhibitors_Full_Description.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Data scraped successfully from {total_pages} pages and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove duplicate rows with the same company name and combine their stand values into a single row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates merged and data saved to LondonVetShow_Exhibitors.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Load the Excel file with the scraped data\n",
    "input_file = \"LondonVetShow_Exhibitors_Full_Description.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Grouping by Company Name and merge Stand values with comma separated for duplicates\n",
    "merged_df = df.groupby('Company Name', as_index=False).agg({\n",
    "    'Description': 'first',  # Take the first non-null description\n",
    "    'Stand': lambda x: ', '.join(x.dropna().unique())  # Join unique stand values with commas\n",
    "})\n",
    "\n",
    "# Save the file\n",
    "output_file = \"LondonVetShow_Exhibitors.xlsx\"\n",
    "merged_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Duplicates merged and data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The idealogy I used is to determine the total number of pages, then iterating through each page to extract relevant company details (if there is description available to any company then navigate to those companies webpage and collect the complete description), and finally saving the collected data into excel file.  Since there were some duplicate company names with different stands, I implemented logic to group the stand values by the same company name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "https://www.geeksforgeeks.org/how-to-scrape-multiple-pages-of-a-website-using-python/\n",
    "\n",
    "https://www.geeksforgeeks.org/http-headers-user-agent/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "case-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
